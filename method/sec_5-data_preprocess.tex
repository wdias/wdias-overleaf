\section{Data Prepossessing}
\label{se:data_preprocess}

The data imported via import modules may have incorrect data, missing data, and incompatible data which cannot directly feed into simulation modules.
Thus, we need to preprocess imported data before using them for weather forecasting. The WDIAS supports data preprocessing capabilities via extension modules. The current system has a few inbuilt extensions such as interpolation, transformation, and validation, but users can add more extensions as required.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.6\textwidth]{method/data_preprocess/weather_data_preprocessing.pdf}
    \caption{A generic mathematical function model for weather data preprocessing.}
    \label{fi:weather_data_preprocessing}
\end{figure}

The data preprocess functionalities support simple generic mathematical function models. Each extension is considered as a mathematical function that can take \texttt{p} input timeseries variables and output \texttt{n} timeseries variables after applying the given function. Further, the system supports to provide a bind constant as configurations to the extension at the time of preprocessing. Thus, users can change the behavior of an existing function by providing different bind constants at the time of creating new triggers for an extension and produce different behaviors. Next, we explain how to derive interpolation, transformation, and validation functions using the generic mathematical function model described above.

\subsection{Interpolation}
The interpolation extension generates data at desired locations or desired data points using a serial or spatial interpolation technique. It is used to fill gaps in linear measured data and to derive spatially distributed data for weather timeseries, such as precipitation and temperature based on information available at nearby locations. Here, we will discuss two types of interpolation, such as serial interpolation and spatial interpolation, below.

\subsubsection{Serial Interpolation}
During serial interpolation, we use interpolation to fill any gaps in a linear timeseries. The interpolation module will only consider a single timeseries, which is itself filling the missing data points. For example, as seen in Figure \ref{fi:serial_interpolation}, there can be timeseries that has data points of temperature collected using a weather station. There can be missing data in this timeseries, and we can use a simple linear interpolation algorithm to fill the missing values based on the existing data points.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\textwidth]{method/data_preprocess/serial_interpolation.pdf}
    \caption{Serial interpolation.}
    \label{fi:serial_interpolation}
\end{figure}

\subsubsection{Spatial Interpolation}
During spatial interpolation, we use interpolation to either fill gaps in timeseries or create a new timeseries for a location using data from other (spatially distributed) locations' timeseries. Spatial interpolation can also be applied for sampling scalar timeseries from grid timeseries, for re-sampling grids, or for creating grids from timeseries data. For example, let us assume that there are precipitation timeseries data available for location \textit{A} and location \textit{B} as shown in Figure \ref{fi:spatial_interpolation}. Then we want to calculate the mean precipitation of nearby locations using these two timeseries. We can use the spatial interpolation to get the mean precipitation of nearby locations.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\textwidth]{method/data_preprocess/spatial_interpolation.pdf}
    \caption{Spatial interpolation.}
    \label{fi:spatial_interpolation}
\end{figure}

\subsection{Transformation}
Simple arithmetic manipulation of time interval transformation and shifting the series in time-specific hydro-meteorological transformation such as stage-discharge relationships are considered transformation. For example, as seen in Figure \ref{fi:transformation}, there can be a timeseries of precipitation with 1-minute sample data. There will be a model requirement that needs hourly data. Therefore, we need to accumulation data manipulation to create a timeseries of hourly data using 1-minute sample data.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\textwidth]{method/data_preprocess/transformation.pdf}
    \caption{Time interval aggregation.}
    \label{fi:transformation}
\end{figure}

\subsection{Validation}
Validation functions check for counting reliable, doubtful, unreliable, and missing values. The weather stations can produce invalid observations due to technical errors in the sensors of the devices. If we feed those values directly to the models, the models will produce inaccurate forecasting. To avoid that, we need to modify or flag those values before feeding them to the numerical models. As seen in Figure \ref{fi:validation} We can use validation extensions to modify or flag invalid data. For example, a weather station can record temperature above 70 celsius due to sensor error or calibration issues. However, in the real world, it is not possible to have such a temperature in nature. Thus, we need to flag such temperature issues and repair the weather station as soon as possible.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\textwidth]{method/data_preprocess/validation.pdf}
    \caption{Timeseries data validation.}
    \label{fi:validation}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extension API for Data Prepossessing}

We exposed an \acrshort{api} to create new triggers for extensions via /extension endpoints. Following is the accepted payload of the request while creating a new trigger. Later we discussed the \acrshort{api} in detail and how we can use the \acrshort{api} to create new triggers to use extension modules.

\begin{lstlisting}[language=Python]
{
    "extensionId": "",
    "extension": "Interpolation/Transformation/Validation",
    "function": "",
    "variables": [
        {
            "variableId": "",
            "metadata/metadtaIds": {
            }
        }
    ],
    "inputVariables": [],
    "outputVariables": [],
    "trigger": [
        {
            "trigger_type": "OnChange/OnTime",
            "trigger_on": []
        }
    ],
    "options": {
    }
}
\end{lstlisting}
\begin{itemize}
    \item \emph{extensionId} -- An unique identifier for new extension trigger. Should be unique among all extensions.
    \item \emph{extension} -- The main extension category which is responsible for handling the extension with extracting data required and storing the output. Interpolation/Transformation/Validation support for the moment.
    \item \emph{function} -- Name of the microservice which is handling the input to output mapping. \texttt{Fn()} function in the \cref{fi:weather_data_preprocessing}.
    \item \emph{variables} -- Array timeseries mapping to variables. Multiple variables can be define with metadata or metadataIds of timeseries. Can be use same variable on inputVariables and outputVariables below.
    \item \emph{inputVariables} -- Timeseries that need to be provide into the function.
    \item \emph{outputVariables} -- Timeseries that output from the function.
    \item \emph{trigger} -- Type of trigger for the extension. Should be one of OnTime or OnChange.
        \begin{itemize}
            \item \emph{OnChange}: \textit{trigger\_on} --- List of timeseries to listen on change and trigger the extension.
            \item \emph{OnTime}: \textit{trigger\_on} -- List of schedules that need to be trigger the extension in cronjob string format.
        \end{itemize}
    \item \emph{options} -- Bind Constant which is bind at the time of creating new extension trigger and pass on triggering the extension.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Extension Handler}

Extension handler is responsible for triggering the correct extension with timeseries metadata when there is any change in the subscribed timeseries which are provided as a list of \emph{OnChange} timeseries while creating an extension trigger.
When new data is imported into one of the scalar adapter or vector adapter, those microservices notify the extension handler. Then the extension handler checks whether there is any extension subscribed for imported timeseries. If there is an extension needed to be triggered, then it invokes the given extension with the required data.

To improve the performance of matching triggers against a timeseries, we used two mechanisms as below. First, other than only storing extension metadata in a \acrshort{rdbms} instance of extension adapter, we also store extension data in a reverse lookup table called triggers while creating new extension triggers. Using this reverse lookup table, we can retrieve the extensions against a given timeseries without heavy processing and less delay. Second, we cache the resulting trigger type and timeseries metadata using the Redis database inside the extension handler for fast access and avoid too many queries on the \acrshort{rdbms} database. For the separation of service responsibilities, the extension data stored in the persistence database of the extension adapter and exposed via an endpoint.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extension Scheduler}

Extension scheduler is responsible for triggering the correct extension with timeseries metadata at a given time that provided while creating an extension trigger as \emph{OnTime} cron job values. To improve the performance of triggering cron jobs at given time schedules, we used a few optimization mechanisms as described below. 
Same as for extension handler, the extension adapter exposed an endpoint to retrieve extension metadata grouped by cronjob time. Then we create cron jobs as per the data retrieved via extension adapter. We used a low-level programming language to simulate the cron jobs for fast processing the extensions. After getting extension triggers, the extension handler grouped them based on cronjob time and scheduled cron jobs programmatically to trigger at given times. 
When each cron job triggers at the scheduled time, each cron job also contains the extensions to be triggered. Fetching extension triggers data, and creating cron jobs occur with a long period to avoid huge overhead on extension adapter. When a user creates a new extension trigger, those data push into a queue in the Redis database. Then the extension handler pulls data from the queue in regular time intervals and creates new cron jobs for mentioned cronjob time. 
As mentioned above, to avoid the expansion of the programmatic cron job list that we created for new extension triggers, all cronjobs fetch again after a long period and flush the previously scheduled cron jobs. While doing that, we also flush the Redis queue.
The extension handler should be run as a single process for the atomicity, to avoid triggering the same extension twice.