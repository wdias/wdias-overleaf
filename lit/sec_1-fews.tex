Many weather forecasting, assimilation, and dissemination systems are developed to reduce the damage causing by natural disasters such as floods, storms, hurricanes, and even droughts. %Such natural disasters heavily affected on the economy and the life conditions of the country, researchers have developed systems and many of these systems are using as proprietary systems. 
Other countries are also using those systems while adopting them %with own version of system which effective for the 
to specific weather conditions.% in a particular country.
This chapter presents a literature review on existing systems and their architecture approach and design. In \cref{se:fews}, we present \acrshort{fews}. \acrfull{lead}, an open data handling platform distributed as closed-source software, is presented in \cref{se:lead}. % \acrshort{lead} is following the \acrfull{soa} for system architecture. 
In \cref{se:dias}, we discussed \acrfull{dias} which is a common data platform that can work with weather data. \acrfull{madis} is another widely used data integration system, which also provides data access with quality control, which we discuss in \cref{se:madis}.
Under each system, we explore both system architecture, scalability, and flexibility of the system.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\acrshort{fews} flow forecasting system}
\label{se:fews}

\acrshort{fews} \cite{Werner2013TheSystem} is developed by Deltares in the Netherland with targeted for use by operational forecasting agencies. The codebase of \acrshort{fews} is currently not fully open source.
Most, if the models are using a model-centric approach such as inputs, need to be in the format of specific to the model. Also, the outputs produced by the model are specific to the model and hard to utilize by other systems (e.g., FLO2D). As \acrshort{fews} uses a modular approach it is easier to integrate new models. 
Thus, \acrshort{fews} can be considered as an integration framework or a middleware for the models.

Forecasting processes are constructed as a combination of modelling steps and data transformation algorithms. These are then combined to the provide required forecast capabilities. Flexibility is achieved by integrating new models and algorithms into the codebase \cite{Werner2013TheSystem}. \acrshort{fews} system contains no inherent hydrological modeling capabilities within its codebase. Rather it is a framework that can use to integrate into the system and create workflows for forecasting.

Are proposed by Haggett \cite{Haggett1998AnWales} key elements of a forecasting system include detection, forecasting, dissemination and warning, and response. Within these four steps, \acrshort{fews} focuses on the forecasting step. The primary objective of this step is to provide additional lead time through predictions of short term future hydro-meteorological conditions \cite{Werner2005FloodCatchments}. It is a valid argument that providing accurate predictions with greater lead time can reduce the level of destruction. To do the forecasting, the system should be capable of integrating real-time data from hydrological and meteorological observation networks, and the dissemination of prediction results through appropriate products to the warning process.

In \db{Figure} \cref{fi:fews_schematic} show a schematic view of the connection between the forecasting system to real-time data acquisition systems and dissemination systems. The widely used concept is using meteorological forecast data to get precipitation and, then using a hydrological and hydraulic model chain to predict the level affect on the ground. The hydrological and hydraulic model should be design based on the ground. The ground should be analyzed geographically and divide into the catchments. Based on the affected catchment, it can be further divided into sub-catchments to reduce the complexity of the simulation task. Ideally, the forecasting system should be flexible to allow change to models and data, while keeping the way forecasters work with it as constant as possible.
\dbc{F is capital and T is capital when referring to Figures and Tables. Fix all other places.}
\gkc{FIXED}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\textwidth]{lit/fews/Schematic-structure-of-a-fl-ood-forecasting-system-showing-the-position-of-Delft-FEWS_W640.png}
    \caption[Schematic structure of a flood forecasting system including \acrshort{fews} and links to other primary systems within the operational environment]{Schematic structure of a flood forecasting system including \acrshort{fews} and links to other primary systems within the operational environment \cite{Werner2013TheSystem}.}
    \label{fi:fews_schematic}
\end{figure}

The foundation of \acrshort{fews} is data-centric, with a common data-model through which all components interact. All timeseries data (both scalar and gridded) are stored in this common data model in a database. Modeling capabilities are then linked to the system through one of the interfaces provided to the data-model \cite{Werner2013TheSystem}. As mention in the paper, having a common data-model make easy to store data efficiently. Operations like reporting and sharing the data also make it easy by integrating models. But the problem with this approach is, handling the multiple data formats. \acrshort{fews} overcome this issue by introducing adapters for most of the commonly used data formats. This is a good feature that enables users to easily import data into the system. But at the same time, it's adding additional complexity to the system.

Timeseries data is additionally either scalar, vector, or gridded data, though all different types are uniformly stored as binary objects in a time series table. None of the functional components (including the linked models) have direct access to the times series table, which is accessible only through the data access module \cite{Werner2013TheSystem}. As explained in the system interpretation, every time all the data access needs to go through the data access module which causing adding more cost in data access. Since it is storing the different data types as binary objects, there is a penalty in converting data into binary objects and vice versa. Storing all the data in a timeseries table causing all the requests to come into a single data point. Even it gives the advantage of accessing all the data in one place. Since screaming big amount of data is causing a heavy load on the system, causing to slow down in the performance of trying to use the system on a large set of data.

Given above drawback on storing the data in the system, within the \acrshort{fews} data model, time series are uniquely identified by their location and data type, as well as an id related to the source of the data (e.g., the external source or hydrological model of which the time series are a result) \cite{Werner2013TheSystem}. This allows indexing the database on the above key fields and stores the data separately, to take advantage of putting multiple data resources into the system. As an example, instead of storing the data at a single timeseries table, it is possible to separate and index the database based on the source such as an external source or hydrological model of which the timeseries is a result. Or separate by data type and store in multiple storages give the capability for the system to scale with a factor of identical types that can be identified. Example of separation by data types of scalar, vector and gridded will increase the scaling factor by 3x.

Data processing and manipulation is a required process in weather forecasting. Most of the data that is imported from external sources is not at the appropriate temporal and spatial scale to be applied as an input to a forecasting model, or to be used directly in product generation. As a consequence, generic data processing steps form the predominant effort in most applications of models in the forecast environment. Some examples include data validation, serial and spatial interpolation, aggregation and disaggregation, and merging data \cite{Werner2013TheSystem}. This a vital feature in a forecasting system, and affect the quality and accuracy of the predicted data outputs. Because of the common data model concept in \acrshort{fews}, data processing via these functions is much effective. The system is self-provided some of the generic functions for data processing, but for more complex algorithms can be developed as a new Java class coded to communicate with the \acrfull{api}. It is obvious that for additional feature integration, users should implement the new functional extensions via Java programming language. Users do not have the flexibility of developement with some other languages they are familiar with or use the support from another language. As an example, Python language is easy to use for beginners and many data scientists are using this programming language for development and it has many data available libraries for data processing. But in \acrshort{fews}, users cannot take advantage of such existing tools.
\dbc{Define Abb. for API in Chapter 1.}
\gkc{FIXED}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1.0\textwidth]{lit/fews/Architecture-of-Delft-FEWS-showing-the-data-base-the-data-access-layers-and-examples-of_W640.png}
    \caption[Architecture of \acrshort{fews}]{Architecture of \acrshort{fews} \cite{Werner2013TheSystem}.}
    \label{fi:fews_data_layer}
\end{figure}
Most of the library of functions provided work equally on scalar and gridded time series. As with all other modules, this communicates with the database solely through the data access layer as shown in \cref{fi:fews_data_layer} \cite{Werner2013TheSystem}. As it is shown in the figure, it is clearly showing that the system depends on a single database and via the data access layer, all the requests are coming to the database through a connection. It is possible to setup and connect to an enterprise-level database with clustering and shading as a paid solution, to serve many requests as possible. But the design itself inherently suffers from the database bottleneck which effected by running a large set of timeseries or models.
\dbc{When referring to a section you need to use the word "Section" as highlighted above. Fix all other places.}
\gkc{FIXED}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.8\textwidth]{lit/fews/Linking-Delft-FEWS-with-external-models-The-fi-gure-shows-the-fl-ow-of-data-through-XML_W640.png}
    \caption[Linking \acrshort{fews} with external models]{Linking \acrshort{fews} with external models \cite{Werner2013TheSystem}.}
    \label{fi:fews_general_adapter}
\end{figure}
One of the simple and most effective features in the \acrshort{fews} is an open approach to the integration of models and data. This approaches the concept of the open modeling framework proposed by Open model integration \cite{Kokkonen2003InterfacingXML}. It simply gives the flexibility to allow operators to integrate more models as well as variations of the same model and come up with new integration flows as much as possible.
\acrshort{fews} generates the input data as a set of XML files to a defined location; an adapter developed specifically for the model in question transforms this to the required native format in a pre-processing step; \acrshort{fews} executes the model, and the adapter to that model then converts the native formatted results into XML formatted files in a post processing step. \acrshort{fews} subsequently imports the results into the database from the XML files (see \cref{fi:fews_general_adapter}) \cite{Werner2013TheSystem}. The process is simple and consistent throughout all the model integration. The model execution is done by the \acrshort{fews} or the model adapter which is causing tide coupling into the execution process in the system. Users do not have the flexibility to run the models with different configurations such as running with parallel execution. That may be overcome by triggering an external process at the execution time, but it introduces more difficulty for handling after model executed successfully, preceding to the next step in the process. This part of the system feature is not focused on the \acrshort{wdias} and user has to come up with own flow or use existing scientific flow management system. But it is a concept that needs to be discussed and understand properly to design a data integration and assimilation system.

Exchanging data with the model is primarily through XML files. In some cases, these XML files may become very large, which may lead to I/O bottlenecks and subsequent performance issues \cite{Werner2013TheSystem}. This issue is thoroughly discussed in previous paragraphs and the authors of the \acrshort{fews} \cite{Werner2013TheSystem} seem to be noticed and accept it here. For overcoming this issue they introduced the file-based exchange of data. This includes the use of binary-XML files, streaming files through memory, and the use of \acrshort{netCDF} files. But far as for the users, it seems to be adding more complexity and context to get higher performance via the system.

If a particular adapter for a model is not present in the list of \acrshort{fews} adapter, the user has the flexibility to implement an adapter for it. The effort of developing an adapter for a model code not previously integrated with \acrshort{fews} will vary depending on the complexity of the model I/O formats \cite{Werner2013TheSystem}. If the users want to use an existing adapter and need to configure something not supported via the existing adapter, it is hard to get done. Also, there are some cases it is hard to develop an adapter or running alongside the \acrshort{fews}. As an example, the FLO2D model is using for hydrologic modeling and the model only supports only on Windows based operating systems. If you setup the system on Linux based operating system and using Linux based models, it is hard for users to come up with a solution for integrating the FLO2D models.
The final step and one of important feature is exporting the data from the system in useful manner.
The forecasting process is often a sequence of steps, starting with the import of data, a several data processing, and modeling steps, and culminating in the generation of products to be disseminated to the warning process. This is supported via the workflow process in \acrshort{fews}. The scope of the \acrshort{wdias} is not focused on adding such a feature in the system, rather user has to use a scientific workflow management system or come up with own version of workflow.

When running sequential steps within a workflow in \acrshort{fews}, each step is run for the full-time window required (e.g., the lead time of the forecast) before moving on to the next step. This precludes tightly coupling of models, even explicitly \cite{Werner2013TheSystem}. As mentioned above, users can run multiple models parallel or run those sequentially as needed. 
