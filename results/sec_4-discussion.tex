\section{Performance Metrics Analysis}
\label{se:discussion}

This section concludes the observations in \cref{se:observations} section against the performance metrics defined in \cref{subse:test_plan_metrics}.

During each test plan, we showed the \emph{latency} for data insertion, and retrieval operation type kept constant over the whole test plan run time without any significant change. When we increased the request size from 60-minute resolution data to 15-minute resolution data, the latency also increased throughout the whole test plan with a smaller number.
During \cref{subse:obs_test_plan_all_auto_15min} test run with auto-scaling, the performance of the grid data got better when compared to \cref{subse:obs_test_plan_all_15min} without auto-scaling. Thus, we can assume by adding more resources to the \acrshort{wdias}, it can handle more workload on the system by providing higher throughput.

While keeping the latency constant without significant change, we showed the \emph{throughput} of the \acrshort{wdias} kept constant while increasing the request size from 60-minute resolution data (24 data points) to 15-minute resolution data (96 data points) for all the data types such as Scalar, Vector, and Grid. When the number of active threads increased, the \acrshort{wdias} able to provide the same throughput while maintaining the latency stable.

One of the reasons for having deviations at the peak time due to uses of a single database instance for the data consistency such as using InfluxDB for scalar and vector data, and netCDF with parallel access support for Grid data. Further, it is possible to increase the performance of the before-mentioned bottleneck places such as using the InfluxDB commercial cluster support for high availability or horizontal scaling of InfluxDB. By using the InfluxDB cluster, we can run multiple pods of adapter-scalar and adapter-vector to support more workload.

Adapter-grid is using a Python wrapper for netCDF FORTRAN implementation with parallel IO enabled. However, it has some performance issues as well as memory leak issues. Since each microservice in the \acrshort{wdias} is independent of technology, we can use a different technology stack to implement the service. Doing so, we can increase the performance as an example by directly porting the adapter-grid to netCDF with a low-level language such as C or FORTRAN. Other than \acrshort{eks}, users can use a different \acrshort{k8s} cluster that supports Persistent Volumes with the access of Read Write Many \cite{LinuxFoundationPersistentKubernetes}. Then they can run multiple pods of adapter-grid and increase the throughput of handling grid data.

When we looked into the \emph{resource utilization} of \acrshort{wdias}, by using the \acrshort{k8s} as the container orchestration system, we were able to scale up and cool down the system as required based on the workload. During the test plan with auto-scaling enabled in \cref{subse:obs_test_plan_all_auto_15min}, we demonstrated the system gets scale up to the maximum at the peak time. Then cool down to a single pod after finishing the test cases. Given the above facts, we can run the \acrshort{wdias} on a cluster from 1 CPU node to nodes with 100 CPUs. As described in \cref{se:microservice}, we used many of the concepts of modern microservice architecture to create stateless, failover, redundant microservice to achieve such capabilities.

The \acrshort{wdias} supports \emph{auto-scaling} out of the box with \acrshort{k8s} support. Services can be configured with a maximum number of pods to avoid over resource usage. When there is not much workload on the system, the system cools down to fewer pods to save more resources. When there is an issue with a pod, \acrshort{k8s} auto-schedule a new pod and remove the unhealthy pod. Also, it allows updating the system without any downtime with rollback updates. By doing that, the \acrshort{k8s} create new pods with upgraded microservice and forward the traffic to new pods after spawning enough new pods, and terminate the old pods.

The \acrshort{wdias} processed many requests with a larger request size than the normal usage with a lower rate of failures to process the requests, mainly with insert Grid data. If we want to \emph{reduce the risk of unable to process data}, then the system can be configured to run with redundant pods to handle spikes of workloads. Also, while configuring the auto-scaling, the \acrshort{k8s} can be configured to maintain a lower amount of CPU usage, such as 50\% to 60\% rather than 80\%. Such configuration will always spawn new pods to handle double of current peak load that the system can handle at a given time.
