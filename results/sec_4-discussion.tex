\section{Performance Metrics Analysis}
\label{se:discussion}

This section concludes the observations in \cref{se:observations} section against the performance metrics defines in \cref{subse:test_plan_metrics}.

The \emph{Latency} for each operation type kept constant over the whole test plan run time without any significant change. When the request size increased from 24 data points to 96 data points, the latency increase throughout the whole test plan with a smaller number. But for each test run, the latency kept constant over time.
During the \cref{subse:obs_test_plan_all_auto_15min} test run, the performance of the Grid data got better when compared to \cref{subse:obs_test_plan_all_15min}. This means by adding more resources to the \acrshort{wdias}, it can handle more workload on the system.

While keeping the latency constant without significant change, the \emph{throughput} of the \acrshort{wdias} kept constant while increasing the request size from 60min data (24 data points) to 15min data (96 data points) for all the data types such as Scalar, Vector, and Grid.
When the number of active threads increased, the \acrshort{wdias} able to provide the same throughput with maintaining the latency stable.

One of the reasons for having deviations at the peak time due to uses of single instance for the data consistency such as  InfluxDB for Scalar and Vector data, and netCDF with parallel support for Grid data. Further, it is possible to increase the performance of these bottlenecks much higher such as using the  InfluxDB Commercial cluster support for high availability or horizontal scaling of  InfluxDB. With  InfluxDB cluster, it is possible to run multiple pods of adapter-scalar and adapter-vector for support more server hits per second.

Adapter-grid is using a Python wrapper for netCDF FORTRAN implementation with parallel IO enabled. This has some performance issues as well as memory leak issues. Since each microservice in the \acrshort{wdias} independent of technology that can use to implement the service, it will increase the performance if the adapter-grid directly ported to netCDF with a low-level language such as C or FORTRAN.
Other than \acrshort{eks}, if users can use a \acrshort{k8s} cluster that supports Persistent Volumes with the access of Read Write  Many \cite{LinuxFoundationPersistentKubernetes} , then it is possible to run multiple pods of adapter-grid and increase the throughput of the \acrshort{wdias}.

When looking into the \emph{resource utilization} of \acrshort{wdias}; since it is using the \acrshort{k8s} as the container orchestration system, it allows us to scale up and cool down the system as required based on the workload. This demonstrates the test plan of \cref{subse:obs_test_plan_all_auto_15min}, and the system gets scale up to the maximum at the peak time. Then cool down to a single pod after finishing the test cases.
Given above \acrshort{wdias} can run from 1 CPU node to nodes with 100 CPUs. As described in the \cref{se:microservice}, it uses many of the concepts of modern microservice architecture to create stateless, failover, redundant microservice to achieve such capabilities.

The \acrshort{wdias} supports \emph{auto scaling} by out of the box with \acrshort{k8s}. Services can configure with a maximum number of pods to avoid over resource usage. When there is not much workload on the system, the system cools down to fewer pods to save more resources. When there is an issue with a pod, \acrshort{k8s} auto-schedule another pod and remove the unhealthy pod. Also, it allows updating the system without any downtime with rollback updates.

\emph{Risk of unable to process data} during the test performance, the \acrshort{wdias} processed many requests with higher request size than the normal usage with a lower rate of failures to process the requests, mainly with insert Grid data. If the usage of \acrshort{wdias} wants to reduce the risk of unable to process data, then the system can configure to run with redundant pods to handle spike of workloads. Also while configuring for the auto-scaling, the \acrshort{k8s} can configure to maintain a lower amount of CPU usage such as 50\% to 60\% rather  than 80\%. Such configuration with always spawn new pods to handle double of current peak load.
