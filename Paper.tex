\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[acronym]{glossaries}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{biblatex}
\usepackage{adjustbox}

\usepackage{todonotes}
\newcommand{\db}[1]{\textcolor{blue!40}{#1}}
\newcommand{\dbc}[1]{\todo[author=Dilum, inline, color=blue!40]{#1}}

\usepackage{hyperref}
\usepackage[noabbrev,capitalise]{cleveref}

\addbibresource{mendeley.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{code-style}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}
\lstset{style=code-style}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Weather Data Integration and Assimilation System\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Gihan Karunarathne}
\IEEEauthorblockA{\textit{Dept. Computer Science and Engineering} \\
\textit{University of Moratuwa}\\
Katubedda, Sri Lanka \\
gihan.09@cse.mrt.ac.lk}
\and
\IEEEauthorblockN{H.M.N. Dilum Bandara}
\IEEEauthorblockA{\textit{Dept. Computer Science and Engineering} \\
\textit{University of Moratuwa}\\
Katubedda, Sri Lanka \\
dilumb@cse.mrt.ac.lk}
\and
\IEEEauthorblockN{Srikantha Herath}
\IEEEauthorblockA{\textit{Center for Urban Water, Sri Lanka} \\
%\textit{Center for Urban Water, Sri Lanka}\\
Battaramulla, Sri Lanka \\
admin@curwsl.org}
}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
%We describe our experience with implementing an extendable open source Weather Data Integration and Assimilation System, which is known as \acrshort{wdias}. It uses the modern architecture pattern such as microservice in order to achieve the scalability, rather using monolithic architecture or client service architecture. It provides a modular approach to integration data from different sources and export into different formats. Also the inbuilt extension module system allow users to add new features. The open source tools that are using for \acrshort{wdias} allows to run on Cloud Computing platforms without much hassle with the auto-scaling feature allow to run \acrshort{wdias} from 1 CPU node to nodes with few hundred CPUs. The paper describes the initial design goals, evolve over few architectures to get desired performance, and explains how the system in way to achieve scalability. Later we analyse the performance test observations with the performance metrics.

\db{Numerical Weather Models (NWMs) utilize data collected via diverse sources such as automated weather stations, radars, air balloons, and satellite images. Prior to using such multimodal data into a NWM, it is necessary to transcode data into a format that can be ingested by the NWM. Moreover, the data integration system’s response time needs to be relatively low to forecast and monitor time-sensitive weather events like hurricanes, storms, and flash floods that require rapid and frequent execution of NWMs. The resulting weather data also need to be accessed by many researchers and third-party applications. Even though there are several weather data integration systems, they are based on monolithic or client-server architectures and are unable to benefit from novel computational models such as cloud computing and containers. Moreover, most of this software is proprietary or closed sourced; hence, it is difficult to customize such software for an island like Sri Lanka with different weather seasons. Therefore, in this research, we propose Weather Data Integration and Assimilation System (WDIAS) that utilizes modern architecture pattern such as microservices to achieve scalability, high availability, and low-cost operation based on cloud computing. WDIAS provides a modular architecture to integrate data from different sources, export into different formats, and the inbuilt extension module system allow users to add new features. We demonstrate the utility of WDIAS using cloud-based experimental setup and weather-related synthetic workloads.}
\end{abstract}

\begin{IEEEkeywords}
\db{Cloud computing, data assimilation, data integration, }
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{pse:Introduction}
Weather prediction is essential to reduce impact due to natural disasters and to gain
effectively manage natural resources like water.
Center for Urban Water, Sri Lanka (CUrW-SL) [3] is tasked with the mission to reduce water-related natural disasters in Sri Lanka.
It collected real-time data from weather stations and other government entities and generates daily forecasts. Then the generated results are shared with relevant public departments responsible for disaster planning, mitigation, and management.
To enhance the accuracy of weather predictions, it is necessary to provide reliable and
detailed weather data as inputs to NWMs.
Before feeding such diverse data collected from different sources, it is necessary to a data format that can be ingest by the NWMs.
Further these data need to be process with less time, and need to handle bulk stream weather data.

There are many forecasting systems developed with considering above factors for disaster management. The \acrshort{fews} is developed in the Netherland by Deltares for operational forecasting. It uses a model-centric approach with considering forecasting process as a combination of data modeling steps and data transformation algorithms. So, it is a flexible framework with no inherent hydrological modeling capabilities and system and capable of creating forecasting workflows by integrating new models and algorithms. Also it follows a common data-model and allow models to interact with the system via one of the interfaces provided, and all the timeseries data are stored via this common data model. Even this approach is efficient in data storing and accessing, but it has the inherent issues of performance of accessing data via a single data model in a database. One of interesting concept of \acrshort{fews} is using unique fields to identify timeseries such by their location and data type, as well as an id related to the source of the data. Even it provide an simple but most effective feature by following the concept of the open modeling framework proposed by Open model integration \cite{Kokkonen2003InterfacingXML}, the model adapter is causing tide coupling into the execution process with the \acrshort{fews}. This causing some issue with unit testing models separately and does not easily allow users to run the models in flexible manner such as parallel execution or external process.

\acrfull{lead} is using the fundamental concept of  dynamic workflow orchestration and
data management in a Web services framework \cite{Droegemeier2005Service-OrientedWeather}. To predict and analyze weather models
by researchers, it required many resources. Rather than each researcher is running and
handling their computer resources to do the weather experiments, LEAD is providing pool of resources, then the researchers can use this resource a pool to run their
experiment in shorter amounts of time and higher scale. It is following the \acrfull{soa} with different service layers, and expose services via drag and drop interface to create new flow by users to run new forecast workflows. Workflow service in the \acrshort{lead} schedule and run those with promising the full use of underline resources in the system. \acrfull{dias} and \acrfull{madis} mostly provide the functionality of integration and sharing of data. When compared to other systems, those do not have the workflow capabilities. To store on the large volume of disk space, \acrshort{dias} converts the data via a provided set of \acrshort{api}s and provide the functionality of quality control of the data, and metadata management and  shares the data via the Shared API layer. \acrshort{madis} also store the data by converting the data into a common format, and provide access to the data based on role access.

Even though there are many data integration systems, most of them are proprietary or closed source.
For an island like Sri Lanka that having different kinds of weather seasons
over the year than that software originated need to be highly customized.
Thus it required to get support from the vendor to integrate and configure the
system.
And most of the software existing out there is not up to date with the latest
technology concepts such as Cloud Computing.
The modern architecture pattern such as microservice architecture was able to provide high scalability and availability, rather using monolithic architecture or client service architecture.
Since most of existing systems are dependent underline infrastructure providers, the users do not have much flexibility to decide the best cost-effective way to host it. Above facts has been considered while we proposed the \acrfull{wdias}. The main purpose of this system is to design and implement an extendable weather data integration, assimilation, and dissemination system which is capable of handling bulk data efficiently with providing scalability and high throughput with using open source tools and modern cloud computing technologies.

This paper first provides a brief review on necessity of weather forecasting and some of existing weather data systems. \cref{pse:background} provides an overview of modern architectural patterns used in \acrshort{wdias} and the important components and features of the \acrshort{wdias} discussed in \cref{pse:wdias_architecture}, next \cref{pse:performance_analysis} discusses the test workload performance analysis with comparing with the metrics. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\db{Background}}
\label{pse:background}

\cref{fi:wdia_components} shows the basic functions of a Weather data integration and assimilation system such as Integration, Assimilation, and Dissemination.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{method/misc/weather_data_system_components.jpg}}
\caption{Components of a weather data system.}
\label{pfi:wdia_components}
\end{figure}

The system should capable of integrating data from different source such as satellite data, high end and low end weather station etc in the form of multidimensional spatial and temporal weather data as shown in the \emph{Integration} modules.
Then the system should be able to fulfill weather models varying data requirements and should be able to store large set of redundant model data during the \emph{Assimilation} module.
The users should be able to easy to search into the available that in the system base on timeseries metadata or based on Geo queries, and the \emph{Dissemination} module should be capable of sharing data among different users.

The \acrshort{wdias} is supporting all three above modules. It iterate through few architectures with concerning about the scalability and high availability.
The initial design of WDIAS used the Service Oriented Architecture (SOA). Moreover, it tried to use an Enterprise Service Bus (ESB) to integrate different modules, as ESB is acting as a common layer for all of the modules. However, it turned out that ESB is not suitable for data streaming or bulk data processing and ESB suffers from a single point of failure since it is using a common bus.
In the second design phase of WDIAS, we attempted to use the actor model using the AKKA framework [9] to overcome above ESB drawbacks and followed the microservice architecture. The system architecture redesign with decomposing into microservices and
used actors to implement each microservice in the system.

Microservice architecture is resilient and flexible as one service is independent of another service. And it is highly scalable as demanding services can be deployed in multiple servers. One of the attractive features of microservice architecture is the independent nature of the microservices which allows to choose different technologies and maintain separately. But using AKKA framework for implement microservice architecture has some disadvantages such as unable to choose independent technologies and the actors messaging between different services cause to result in a too-tight code coupling between the services. To overcome these issues, we moved to the concept of container orchestration based microservice architecture.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Microservice Architecture}
\label{psubse:microservice_architecture}

A microservice architecture style is an approach to developing a single application as
a suite of small services, each running in its own process and communicating with
lightweight mechanisms, often via providing a RESTful API. Follow are some of the microservice design patterns mostly used in the context of designing WDIAS architecture.
\begin{itemize}
    \item \emph{Smart Endpoints and Dumb Pipes} - rather than implementing smart endpoints with supporting multiple protocols and complex logic, microservices aim to be as decoupled and as cohesive as possible with only focusing one own domain.
    \item \emph{Database per microservice} - The idea is to keep each microservices persistent data private to that service and accessible only via its API.
    \item \emph{Distributed Transactions over microservices via event processing} - A service publishes an event when its data changes. Other services consume that event and update their data.
    \item \emph{X-axis Scaling} -  running multiple copies of an application behind a load balancer
    \item \emph{Y-axis Scaling} - splits the application into multiple, different services. Each service is responsible for one or more closely related functions. The microservice architecture is an application of Y-axis scaling.
    \item \emph{Z-axis Scaling} - each server runs an identical copy of the code (similar to X-axis scaling). The big difference is that each server is responsible for only a subset of the data.
\end{itemize}

The \acrshort{wdias} is using \acrfull{k8s} as the container orchestration system which is an open-source tool for automating deployment, scaling, and management of containerized applications.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{method/microservice/k8s_architecture_v3.jpg}}
\caption{\acrfull{k8s} architecture.}
\label{pfi:k8s_architecture}
\end{figure}

\cref{fi:k8s_architecture} gives an overall idea on the components of \acrshort{k8s},
\begin{itemize}
    \item Pods -- Cluster of containers that can group other container images in a single unit.
    \item Nodes -- the machines (VMs, physical servers, etc) in a cluster that runs your applications and cloud workflows.
    \item kubelet -- An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.
    \item kube-proxy -- A network proxy that runs on each node in the cluster, and maintains network rules on nodes.
    \item Kubernetes master -- Responsible for maintaining the desired state for your cluster.
    \item etcd -- Consistent and highly-available key value store used as Kubernetes’ backing store for all cluster data.
\end{itemize}
Users can add much as required Nodes into the K8s cluster, and K8s manage and
deploy applications as pods into cluster nodes. Thus it allow users to implement the microservice architecture concepts above easily.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\acrfull{wdias} Architecture}
\label{pse:wdias_architecture}

\cref{pfi:microservice_separation} shows the clear separation of microservices into the modules of the \acrshort{wdias}. As seen in Figure of \cref{fi:wdias_micro_on_demand} and \cref{fi:wdias_micro_async} each circle represents a microservice in the \acrshort{wdias}, and those are implemented as containerized applications. 

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{method/microservice/separation_microservices-v3.jpg}}
\caption{Separation of \acrshort{wdias} microservices.}
\label{pfi:microservice_separation}
\end{figure}

The left side of Figure 3.3
shows the import modules of the WDIAS, and the right side shows the export modules. each import microservice only does the specific
task of converting and forwarding the request to the correct data adapter module. each data type, there is an adapter microservice is running which is optimized to storing
such type of data. Each adapter has an isolated database, and the database is hosted separately for the high performance.
And timeseries metadata stored using RDBMS which gives more performance over retrieving metadata data, and cached with the In-Memory database to fast access. The system
generates a unique identifier for each timeseries, and throughout the WDIAS, other
microservice use it to handle data for fast access.
Export module microservice follows the same concepts and provides the capability
to export the data into required formats of the weather models.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{method/microservice/microservice_architecture-handle_on_async-v3.jpg}}
\caption{microservice architecture - handle asynchronously.}
\label{pfi:microservice_architecture_async}
\end{figure}

as shown in Figure 3.4, when a request
with a larger size comes to the system, it stores the data for asynchronously process
the data and responds with a unique id that can use to verify whether data processed
successfully or not. First it stores the data and publish an event to another service to process it. After successfully process the data by other service, it updates the system status.

The \acrshort{wdias} provides a simple RESTful API to interact with timeseries metadata and other modules.
\begin{itemize}
    \item \texttt{/parameter} - CRUD operations on parameters
    \item \texttt{/timestep} - CRUD operations on time step
    \item \texttt{/location/point and /location/regualr-grid} - CRUD operations on point location and regular grid locations
    \item \texttt{/timeseries} - CRUD operations on timeseries
    \item Import
    \begin{itemize}
        \item \texttt{/import/json/raw} - Import JSON data in raw format
        \item \texttt{/import/csv/raw} - Import CSV data in raw format
        \item \texttt{/import/ascii-grid/upload} - Upload ASCII JSON Grid files
    \end{itemize}
    \item Export
    \begin{itemize}
        \item \texttt{/export/json/raw} - Export JSON data in raw format
        \item \texttt{/export/ascii-grid/binary} - Download ASCII JSON Grid file
    \end{itemize}
    \item Extensions
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\acrshort{wdias} Database Structure}
\label{subse:wdias_database}

The foundation of the \acrshort{wdias} is data-centric and build around the timeseries data.

\paragraph{Timeseries}-- \emph{A timeseries is simply a series of data points ordered in time}. In the weather domain, its interest in timeseries in perspective of observations to forecasting. Each timeseries, the data points can be formed in different formats as well. For example, scalar (0D), vector (1D), grid (2D), and polygon (2D).

Following list is the timeseries metadata which can use to uniquely identify one timeseries from another which are call as \emph{key attributes}.

\begin{itemize}
    \item \texttt{Module ID}--  String field which describe the source of the data generated.
    \item \texttt{Value Type}--  Scalar, Vector, Grid type
    \item \texttt{Location}-- Location of the timeseries. Point location, regular grid location or irregular grid location.
    \item \texttt{Parameter}-- variable measuring against a location
    \item \texttt{Timeseries Type} - Such as Historical or forecast
    \item \texttt{Time Step}-- Unit of Second, Minute, Hour, Day, Week, Month, Year, or NonEqualDistance. One of multiplier or divider can be used to define the interval between each measurement.
\end{itemize}

Scalar data point only consist of a single value. And Vector data point consists of two values such as magnitude and the direction of the Vector. But for Grid data, a point can consist of multiple values. To get the advantage of storing data efficiently, we can use a set of different databases based on the advantage of using them for each valueType. To support search queries with lower latency, it required to use indexing for the fast search for timeseries metadata. Further, Geo-based indexing is required to support Geo-based search queries based on the locations. Above capabilities achieved by using a database structure with compose of multiple databases as shown in \cref{pfi:database_structure}.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.4\textwidth]{method/microservice/wdias_database_structure.jpg}}
\caption{\acrshort{wdias} database structure.}
\label{pfi:database_structure}
\end{figure}

Since metadata consists of multiple key attributes, the data can be efficiently store using a Relational Database Management System (RDBMS). Two instances of Timeseries database is using in the WDIAS system by adapter-scalar and adapter-vector services. Use \acrfull{netCDF} for store the Grid data which support the creation, access, and sharing of array-oriented scientific data. And  in-memory database for caching for fast access the frequent access data in adapter-metadata and adapter-extension. Using a document-oriented database with support the Geo searching capabilities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Weather Data Preprocessing}
\label{psubse:data_preprocessing}

The \acrshort{wdias} provides the data preprocessing capabilities via extension modules with a simple generic mathematical function model.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.4\textwidth]{method/data_preprocess/summary_weather_data_preprocessing.jpg}}
\caption{Generic functional approach to weather data preprocessing.}
\label{pfi:summary_weather_data_preprocessing}
\end{figure}

Each extension is considering as a mathematical function which can take \texttt{p} number of input timeseries variables and output \texttt{n} number of timeseries variables. Other than that, at the time of processing the system can configure to provide bind constant which can provide while configuring an extension. This means it is possible to change the behavior of an existing function by providing different bind constant at the time of creating new triggers for an extension. The system allow to create new triggers and update them at the run time via POST requests.

\begin{lstlisting}[language=Python]
{
    "extensionId": "", // An unique identifier for new extension trigger
    "extension": "Interpolation/Transformation/Validation",
    "function": "", // microservice which handles the function mapping
    "variables": [ // Array of timeseries mapping to variables
        {
            "variableId": "",
            "metadata/metadtaIds": {
            }
        }
    ],
    "inputVariables": [], // Input timeseries
    "outputVariables": [], // Output timeseries
    "trigger": [ // When to trigger the function
        {
            "trigger_type": "OnChange/OnTime",
            "trigger_on": []
        }
    ],
    "options": { // Run time bind contant data for the extension
    }
}
\end{lstlisting}

Extension Handler is responsible for triggering the correct Extension with metadata when there is any change on the subscribed extension trigger's \emph{OnChange} timeseries.
Extension Handler is responsible for triggering the correct Extension with metadata at a given time which is provided at the creation on extension trigger with the value of OnTime cronjob string value. To improve the performance of triggering cronjobs at given time schedules, the WDIAS added few optimization mechanisms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Query Timeseries Metadata}
\label{psubse:query_timeseries}

Other than the metadata manipulation API mentioned in \cref{psubse:microservice_architecture}, the \acrshort{wdias} also supports timeseries search queries and Geo queries. As an example it can \emph{query timeseries in an area} This operation uses \texttt{geoWithin} search operator for the simplicity. Other than it can also support query timeseries in an area by parameter, query available timeseries by locations and query locations within an area etc.
\begin{lstlisting}
    POST <HOST_NAME>/query/timeseries
    JSON Body:
    {
        "geoJson": {
            "type": "Polygon",
            "coordinates": [
                [
                    [
                        <longitude>,
                        <latitude>
                    ],
                    ...
                ]
            ]
        }
    }
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Analysis}
\label{pse:performance_analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Load Testing Plan}
\label{psubse:test_plan}

The test plan is to perform load testing on whole system, and analyze the scalability of the system while measuring the operations perform on each module. Two variables can vary while doing performance testing.
\begin{enumerate}
    \item \acrfull{rps}
    \item Request size
\end{enumerate}

\acrshort{wdias} is getting a mix of all the data types in the percentages of 70\% scalar, 20\% vector, and 10\% grid values. \cref{pfi:performance_study} shows for a given request size the RPS increased in steps. In each step, the RPS holds for a moment to give some time for the system to get stabilized.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{results/work_load/performance_study_v4.jpg}}
\caption{\acrshort{wdias} load testing plan with changing the request size and \acrshort{rps}}
\label{pfi:performance_study}
\end{figure}

Holding the peak load will help to show whether the system is capable of scaling more than the proposed load testing plan. After the peak, the test case goes through a cool-down period to measure the ability to shrink down the resources when there is not any load on the system. During the test plan, it runs for 30minutes of period with changing the request size form 1-hour data to 15 minutes data. Then another 30 minutes load test enabling the auto scaling of container orchestration system, and perform 5minutes test run with timeseries queries. The the results are evaluate against the performance metrics of throughput, latency, resource utilization and auto-scaling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}
\label{psubse:experimental_setup}

During the performance test, it creates 1000 timeseries using the locations are taken from Google's Countries Public Data set \cite{GoogleGoogleCounties}, and uses static 100 Grid locations for store Grid data. Load test perform using \acrshort{jmeter} and used the Distributed Testing feature to generate higher load. It uses the master-slave approach and it allowed to handle all the test cases via single instance, rather handling separate instances. The system is capable of storing any numeric value up to 3 decimal points. It does not make any difference based on the parameter type whether it is the precipitation or the temperature. Thus during the performance testing, \acrshort{jmeter} uses real precipitation data from \acrshort{curw}. One month period of data for five weather stations has been using to prepare the test data.

While implementing the test cases for WDIAS, it uses the Concurrency Thread Group
with Throughput Shaping Timer because it supports the open workload approach.

\begin{itemize}
    \item \emph{closed system model} \cite{Haggett1998AnWales} -- a new request is only triggered by the completion of a previous request, following by a think time.
    \item \emph{open system model} -- new requests arrival independently of completions.
\end{itemize}

\acrshort{wdias} is using the \acrfull{k8s} as the container orchestration system which is an open-source system for automating deployment, scaling, and management of containerized applications. They are many \acrshort{k8s} solutions available as a cloud service, we used \acrfull{eks} based on the availability. \cref{ptab:aws_eks_nodes} shows the list of nodes/computers used during the performance test.

\begin{table}[ht]
\centering
\caption{\acrshort{eks} nodes}
\begin{adjustbox}{width=0.45\textwidth}
\footnotesize
\begin{tabular}{|l|c|c|c|c|l|}
\hline
\textbf{Node Label} & \textbf{vCPU} & \textbf{RAM (GB)} & \textbf{Storage (GB)} & \textbf{Quantity} & \textbf{EC2 Name} \\ \hline
core & 16 & 32 & 15 & 1 & c5.4xlarge \\ \hline
grid & 8 & 16 & 25 & 1 & c5.2xlarge \\ \hline
scalar & 8 & 16 & 20 & 1 & c5.2xlarge \\ \hline
test & 4 & 10.5 & 5 & 1 & c5n.xlarge \\ \hline
\end{tabular}
\end{adjustbox}
\label{ptab:aws_eks_nodes}
\end{table}

To get better performance using the available resources, it is possible to schedule each microservice into a predefined set of nodes. Pods are assigned to the nodes \cref{tab:aws_eks_nodes} based on the node label as below;
\begin{itemize}
    \item \emph{core} -- metadata, extensions, query, status and extension services
    \item \emph{grid} -- grid adapter, import and export grid data
    \item \emph{scalar} -- scalar and vector adapters, import and export of same data types
    \item \emph{test} -- \acrshort{jmeter} and metric server
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance Evaluation}
\label{psubse:performance_evaluation}

While performing the load testing, each test plan performed around 310k of total sample requests within 30minutes. The ratio between Scalar and Vector to Grid operations around 71k:8k with the ratio of 90\%:10\%. This implies that test cases were able to perform as planned in the \cref{psubse:test_plan}. Insert 

\begin{table}[htbp]
\caption{ Throughput and Latency of load testing with 60min data}
\begin{center}
\begin{adjustbox}{width=0.45\textwidth}
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Label} & \textbf{Samples} & \textbf{Avg$^{\mathrm{a}}$} & \textbf{90\%$^{\mathrm{b}}$} & \textbf{S.D.$^{\mathrm{c}}$} & \textbf{Error \%} & \textbf{RPS} \\ \hline
Insert Timeseries & 71826 & 28 & 31 & 58.74 & 0.00\% & 40.5 \\ \hline
Retrieve Timeseries & 71796 & 8 & 10 & 4.18 & 0.00\% & 40.7 \\ \hline
Insert Grid & 7982 & 23 & 26 & 4.23 & 0.06\% & 4.5 \\ \hline
Retrieve Grid & 7979 & 68 & 75 & 10.11 & 0.00\% & 4.5 \\ \hline
Query: Location & 71804 & 3 & 3 & 1.52 & 0.00\% & 40.5 \\ \hline
\textbf{TOTAL} & 311182 & 127 & 503 & 207.80 & 0.00\% & 175.4 \\ \hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Avg: Average Latency, $^{\mathrm{b}}$S.D.: Standard Deviation, $^{\mathrm{c}}$90\%: 90\% percentile.}
\end{tabular}
\end{adjustbox}
\label{ptab:obs_all_60_min_summary_throughput}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{ Throughput and Latency of load testing with 30min data}
\begin{center}
\begin{adjustbox}{width=0.45\textwidth}
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Label} & \textbf{Samples} & \textbf{Avg$^{\mathrm{a}}$} & \textbf{90\%$^{\mathrm{b}}$} & \textbf{S.D.$^{\mathrm{c}}$} & \textbf{Error \%} & \textbf{RPS} \\ \hline
Insert Timeseries & 71759 & 29 & 32 & 50.97 & 0.00\% & 40.5 \\ \hline
Retrieve Timeseries & 71730 & 9 & 10 & 6.04 & 0.00\% & 40.6 \\ \hline
Insert Grid & 7972 & 44 & 49 & 8.17 & 0.08\% & 4.5 \\ \hline
Retrieve Grid & 7971 & 81 & 93 & 15.15 & 0.00\% & 4.5 \\ \hline
Query: Location & 71734 & 3 & 3 & 1.90 & 0.00\% & 40.5 \\ \hline
TOTAL & 310878 & 129 & 0 & 207.10 & 0.00\% & 175.3 \\ \hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Avg: Average Latency, $^{\mathrm{b}}$S.D.: Standard Deviation, $^{\mathrm{c}}$90\%: 90\% percentile.}
\end{tabular}
\end{adjustbox}
\label{ptab:obs_all_30_min_summary_throughput}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{ Throughput and Latency of load testing with 15min data}
\begin{center}
\begin{adjustbox}{width=0.45\textwidth}
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Label} & \textbf{Samples} & \textbf{Avg$^{\mathrm{a}}$} & \textbf{90\%$^{\mathrm{b}}$} & \textbf{S.D.$^{\mathrm{c}}$} & \textbf{Error \%} & \textbf{RPS} \\ \hline
Insert Timeseries & 71775 & 30 & 41 & 51.71 & 0.00\% & 40.5 \\ \hline
Retrieve Timeseries & 71736 & 23 & 32 & 50.18 & 0.00\% & 40.6 \\ \hline
Insert Grid & 7975 & 91 & 112 & 19.58 & 1.42\% & 4.5 \\ \hline
Retrieve Grid & 7972 & 118 & 165 & 56.15 & 0.00\% & 4.5 \\ \hline
Query: Location & 71749 & 3 & 4 & 2.32 & 0.00\% & 40.5 \\ \hline
\textbf{TOTAL} & 310934 & 134 & 503 & 206.40 & 0.04\% & 175.4 \\ \hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Avg: Average Latency, $^{\mathrm{b}}$S.D.: Standard Deviation, $^{\mathrm{c}}$90\%: 90\% percentile.}
\end{tabular}
\end{adjustbox}
\label{ptab:obs_all_15_min_summary_throughput}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Throughput and Latency of load testing with 15min data while enabled K8s
auto-scaling}
\begin{center}
\begin{adjustbox}{width=0.45\textwidth}
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Label} & \textbf{Samples} & \textbf{Avg$^{\mathrm{a}}$} & \textbf{90\%$^{\mathrm{b}}$} & \textbf{S.D.$^{\mathrm{c}}$} & \textbf{Error \%} & \textbf{RPS} \\ \hline
Insert Timeseries & 71727 & 34 & 27 & 118.78 & 0.00\% & 40.5 \\ \hline
Retrieve Timeseries & 71693 & 7 & 9 & 18.72 & 0.00\% & 40.5 \\ \hline
Insert Grid & 7968 & 87 & 98 & 14.07 & 0.18\% & 4.5 \\ \hline
Retrieve Grid & 7965 & 89 & 110 & 37.79 & 0.00\% & 4.5 \\ \hline
Query: Location & 71704 & 1 & 2 & 2.05 & 0.00\% & 40.5 \\ \hline
\textbf{TOTAL} & 310734 & 130 & 501 & 212.35 & 0.00\% & 175.3 \\ \hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Avg: Average Latency, $^{\mathrm{b}}$S.D.: Standard Deviation, $^{\mathrm{c}}$90\%: 90\% percentile.}
\end{tabular}
\end{adjustbox}
\label{ptab:obs_all_auto_15_min_summary_throughput}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{results/obs/all_auto/obs_all_auto_15m_res_latencies_against_hits.png}}
\caption{Performance test 15min data enabled auto-scaling - response latencies against server hits over the elapsed time.}
\label{pfi:test_obs_auto_all_15_min_latency_vs_hits}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{results/obs/all_auto/obs_all_auto_15m_transaction_throughtput_vs_threads.png}}
\caption{Performance test 15min data enabled auto-scaling - transnational throughput vs number of active threads.}
\label{pfi:test_obs_auto_all_15_min_throughput_vs_threads}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance Metrics Analysis}
\label{psubse:performance_metrics}
\paragraph{Latency}
The latency for each operation type kept constant over the whole test plan run time without any significant change. When the request size increased from 24 data points to 96 data points, the latency increase throughout the whole test plan with a smaller number. But for each test run the latency kept constant over the time.
During the \cref{subse:obs_test_plan_all_auto_15min} test run, the performance of the Grid data got better when compared to the \cref{subse:obs_test_plan_all_15min}. Which means by adding more resources to the \acrshort{wdias}, it can handle more workload on the system.

\paragraph{Throughput}
While keeping the latency constant without significant change, the throughput of the \acrshort{wdias} kept constant while increasing the request size from 60min data (24 data points) to 15min data (96 data points) for all the data types such as Scalar, Vector and Grid.
When number of active threads increased, the \acrshort{wdias} able to provide the same throughput with maintaining the latency stable.

One of the reason to having deviations at the peak time due to use of single instance for the data consistency such as \acrshort{influxdb} for Scalar and Vector data, and netCDF with parallel support for Grid data. Further it is possible to increase the performance of these bottlenecks much higher such as using \acrshort{influxdb} Commercial cluster support for high availability or horizontal scaling of \acrshort{influxdb}. With \acrshort{influxdb} cluster, it is possible to run multiple pods of adapter-scalar and adapter-vector for support more server hits per second.

Adapter-grid is using a Python wrapper for netCDF FORTRAN implementation with parallel IO enabled. This has some performance issues as well as memory leak issues. Since each microservice in the \acrshort{wdias} independent of technology that can use to implement the service, it will increase the performance if the adapter-grid directly ported to netCDF with a low level language such as C or FORTRAN.
Other than \acrshort{eks}, if users can use a \acrshort{k8s} cluster which support Persistent Volumes with the access of Read Write  Many \cite{LinuxFoundationPersistentKubernetes} , then it is possible to run multiple pods of adapter-grid and increase the throughput of the \acrshort{wdias}.

\paragraph{Resource Utilization}
Since \acrshort{wdias} using the \acrshort{k8s} as the container orchestration system, it allows to scale up and cool down the system as required based on the workload. This demonstrate on the test plan of \cref{subse:obs_test_plan_all_auto_15min}, and the system get scale up to the maximum at the peak time. Then cool down to single pod after finishing the test cases.
Given above \acrshort{wdias} can run from 1 CPU node to nodes with 100 CPUs. As described in the \cref{se:microservice}, it uses the many of the concept of modern microservice architecture to create stateless, failover, redundant microservice to achieve such capabilities.

\paragraph{Auto Scaling}
The \acrshort{wdias} supports auto-scaling by out of the box with \acrshort{k8s}. Services can configure with maximum number of pods in order to avoid over resource usage. When there is not much workload on the system, system cool down to less pods in order to save more resources. When there is any issue with a pod, \acrshort{k8s} auto schedule another pod and remove the unhealthy pod. Also it allows to update the system without any down time with rollback updates.

\paragraph{Risk of Unable to Process Data}
During the test performance, the \acrshort{wdias} processed many requests with higher request size than the normal usage with lower rate of failures to process the requests, mainly with insert Grid data. If the usage of \acrshort{wdias} want to reduce the risk of unable to process data, then system can configure to run with redundant pods in order to handle spike of workloads. Also while configure for the auto-scaling, the \acrshort{k8s} can configure to maintain lower amount of CPU usage such as 50\% to 60\% rather 80\%. Such configuration with always spawn new pods to handle double of current peak load.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strengths and Limitations}
\label{psubse:limitations}
\dbc{Make this the dicussion and a sub-section of Sec. 4}

\paragraph{Lack of data preprossessing modules}
The extension module enable plugin system to integrate prepossessing modules which can use for process the data which are inserted to the system or based on regular intervals. Also \acrshort{wdias} provide a more generic open interface approach to create more preprocessing modules. Since \acrshort{wdias} is an open source system, it expect more modules to be create by the community. When compared to other system like \acrshort{fews}, current system only have few extension for the testing purpose. This is one of the major area that need to be improved.

\paragraph{Supporting Irregular Grids}
At the moment \acrshort{wdias} does not support Irregular Grids, but it has the API endpoints define for the implementation which can follow the same microservice architecture. The scope of the \acrshort{wdias} does not belong such level of implementation and the scope has reduced by removing such components from the system.

\paragraph{Grid data performance}
As mentioned in the \cref{se:conclusion}, the adapter-grid is using a Python wrapper for netCDF FORTRAN implementation with parallel IO enabled. This has some performance issues as well as memory leak issues. Since each microservice in the \acrshort{wdias} independent of technology that can use to implement the service, it will increase the performance if the adapter-grid directly ported to netCDF with a low level language such as C or FORTRAN. They are some future work that can focus on improving the performance of grid data.

\paragraph{Define infrastructure as code}
Physical nodes for the \acrshort{k8s} cluster created using Cloud Computing provider such as \acrshort{eks}. And defining the each pod resource limitations at the helm chart level. But the infrastructure that need to deploy the \acrshort{wdias} can define as code such as using tools like Teraform which is independent of the Cloud Computing provider. Using such tool, users will able to easy deploy the \acrshort{wdias} on any Cloud provider without much hassle.

\paragraph{Further improve hierarchical database performance}
As further described in \cref{se:conclusion}, it is possible to increase the performance of these bottlenecks much higher such as using \acrshort{influxdb} Commercial cluster support for high availability or horizontal scaling of \acrshort{influxdb}. With \acrshort{influxdb} cluster, it is possible to run multiple pods of adapter-scalar and adapter-vector for support more server hits per second.
Other than that, the \acrshort{wdias} performance can improve via partitioning the timeseries key space into multiple \acrshort{influxdb} instances. The adapter-scalar or adapter-vector can improve to plug multiple instance of \acrshort{influxdb} instances by changing the configurations. As an example, based on the key attribute \cref{subse:timeseries_key_attributes} Timeseries Type, the adapter-scalar can connect to four instance of \acrshort{influxdb} instances such as  External Historical, External Forecast, Simulated Historical, and Simulated Forecast etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}
\label{pse:summary}
\dbc{Summary is too long. Should not be more than a single para}
To enhance the accuracy of the weather prediction, it is required to have reliable weather data for \acrshort{nwm}. Most of existing weather data systems are proprietary or close sourced, or not up to date with the existing cloud computing technologies.
We proposed an extendable open source Weather Data Integration and Assimilation Systems which is known as \acrshort{wdias}. It focused on providing efficiently integrates weather data from different sources with quality control and supporting steaming large size of data. And compatible with current cloud computing technologies and architecture patterns.

First the architecture starts with SOA using a \acrfull{esb}, and then moved to Actor model based architecture. After comparing the disadvantages of using such architecture with the system requirements, final architecture come up with modern microservice architecture.
Based on the microservice architecture patterns and the nature of the weather metadata, the \acrshort{wdias} came up with the idea of hierarchical database structure in order to provide higher performance while store the weather data. The system uses \acrshort{influxdb} timeseries database for storing Scalar and Vector timeseries data, and using \acrshort{netCDF} for storing Grid timeseries data.
Also the system provide a generic open mechanism to integrate new modules as extension in order to enhance the features of the system. This capability enable to integrate weather data preprocessing flows as extension modules.
The extension API provide easy access to create and modify the extension triggers on the fly without stopping the system or any downtime in order to change the configurations.
The system provides extensive timeseries query endpoints to easy search over the system timeseries metadata with supporting Geo based queries.

\acrshort{wdias} performance test is perform using the \acrshort{jmeter} tool's distributed testing capabilities which separately testing each modules. Then the test plans are performed with increasing the request size, and monitored the performance metrics. The system is setup on the \acrfull{eks} as per the configurations of \cref{subse:test_sys_config}.
During the performance test plans, the system were able to provide constant throughput by keeping the latency constant while increasing the workload. Which exhibit the scalability of the system.
With the \acrshort{k8s} auto-scaling, the \acrshort{wdias} were able to elastically adjust the number of pods according to the workload with the given configurations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}
\label{pse:ack}
\db{This research is supported in part by the grant from the Center for Urban Water, Sri Lanka.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{ {./images/} }
\input{glossary.tex}
\printbibliography[title={References}]

\end{document}
